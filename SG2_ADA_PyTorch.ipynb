{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SG2-ADA-PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9jMmUpn4DWRe"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG7ZEc_982io"
      },
      "source": [
        "# StyleGAN2-ADA-PyTorch\n",
        "\n",
        "## Please Read\n",
        "This StyleGAN2-ADA-PyTorch repository (including this Colab notebook) was forked from [Derrick Schultz](https://github.com/dvschultz/stylegan2-ada-pytorch), which was forked from [Nvidia's original repo](https://github.com/NVlabs/stylegan2-ada-pytorch). A huge thank you to Derrick for developing these tools to make GANs accessible for artists and newcomers, and for generously sharing his knowledge and resources **for free**. Without artists/educators like this, we would be spending much more time (and money) getting these things working. If you find that you're using this Colab notebook extensively, and especially if you're using it to make work to display outside the classroom, **_please be sure to credit [Derrick Schultz](https://www.instagram.com/dvsmethid/?hl=en)_**, and I encourage you all to consider signing up for his [Patreon](https://www.patreon.com/bustbright) or [YouTube channel](https://www.youtube.com/channel/UCaZuPdmZ380SFUMKHVsv_AA/join). You can also send him a one-time payment on [Venmo](https://venmo.com/Derrick-Schultz). Derrick also has a Slack channel dedicated to this stuff, which you can access as a Patreon subscriber.\n",
        "\n",
        "~ ~ ~ ~ ~ ~ ~ ~ ~\n",
        "\n",
        "This Notebook was last updated: March 30, 2021 by [Doug Rosman](https://dougrosman.com/).\n",
        "\n",
        "Additional Colab and Python help from Blake Fall-Conroy.\n",
        "\n",
        "~ ~ ~ ~ ~ ~ ~ ~ ~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj4PG4_i9Alt"
      },
      "source": [
        "## Connecting to a GPU instance\n",
        "\n",
        "Let’s start by checking to see what GPU we’ve been assigned. This will also connect the notebook to a GPU runtime, meaning you'll now officially start using your ~8 hours of free GPU time.\n",
        "\n",
        "Ideally the GPU we get is a V100, but a P100 is fine too. Other GPUs may lead to issues."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VVICTCvd4mc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cade278e-ae0b-43dc-ce1d-2aa2e56de568"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-106747dc-8ddd-998a-fbc5-de8c32655a56)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSV_HEoD9dxo"
      },
      "source": [
        "# Setup\n",
        "First, let’s connect our Google Drive account. This is optional but highly recommended. (You should definitely do this).\n",
        "\n",
        "After completing this step, I recommend opening another tab in your browser to your Google Drive folder. Even though you can access your Google Drive from this notebook, it's much easier to work with your files in the regular Google Drive interface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuVPuJmbigRs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ac2b33-e4fa-4ba5-cce1-7d314f1e5548"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTjVmfSK9CYa"
      },
      "source": [
        "## Install repo\n",
        "\n",
        "The next cell will install the StyleGAN repository in Google Drive. If you have already installed it it will just move into that folder. If you don’t have Google Drive connected it will just install the necessary code in Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8ADVNpBh8Ox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b7a197b-3c29-4095-c704-345a3befdab0"
      },
      "source": [
        "import os\n",
        "if os.path.isdir(\"/content/drive/MyDrive/colab-sg2-ada-pytorch\"):\n",
        "    %cd \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n",
        "elif os.path.isdir(\"/content/drive/\"):\n",
        "    #install script\n",
        "    %cd \"/content/drive/MyDrive/\"\n",
        "    !mkdir colab-sg2-ada-pytorch\n",
        "    %cd colab-sg2-ada-pytorch\n",
        "    !git clone https://github.com/dougrosman/stylegan2-ada-pytorch\n",
        "    %cd stylegan2-ada-pytorch\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !mkdir pretrained\n",
        "    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n",
        "else:\n",
        "    !git clone https://github.com/dougrosman/stylegan2-ada-pytorch\n",
        "    %cd stylegan2-ada-pytorch\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !mkdir pretrained\n",
        "    %cd pretrained\n",
        "    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU\n",
        "    %cd ../\n",
        "\n",
        "!pip install ninja opensimplex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\n",
            "Collecting ninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/de/393468f2a37fc2c1dc3a06afc37775e27fde2d16845424141d4da62c686d/ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 8.1MB/s \n",
            "\u001b[?25hCollecting opensimplex\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/ad/9b758f9ff9dcd23fc574bb3aa1de844adb1179c9be9711e9f798614d4b2f/opensimplex-0.3-py3-none-any.whl\n",
            "Installing collected packages: ninja, opensimplex\n",
            "Successfully installed ninja-1.10.0.post2 opensimplex-0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jMmUpn4DWRe"
      },
      "source": [
        "### **_Do not run this cell unless explicitly told to by Doug_**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV9bdvzeDRPd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6499db22-e76d-42a8-e3b7-b7ab424c2931"
      },
      "source": [
        "%cd \"/content/drive/My Drive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n",
        "!git config --global user.name \"dougrosman\"\n",
        "!git config --global user.email \"drosman@saic.edu\"\n",
        "!git fetch origin\n",
        "!git pull\n",
        "# !git checkout origin/main -- train.py generate.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n",
            "From https://github.com/dougrosman/stylegan2-ada-pytorch\n",
            "   3330aa3..beca8f2  main       -> origin/main\n",
            "error: You have not concluded your merge (MERGE_HEAD exists).\n",
            "hint: Please, commit your changes before merging.\n",
            "fatal: Exiting because of unfinished merge.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZkcJ58P97Ls"
      },
      "source": [
        "# Dataset Preparation - Uploading your data set\n",
        "\n",
        "Upload a .zip of square images to the `datasets` folder. Previously you had to convert your model to .tfrecords. That’s no longer needed :)\n",
        "\n",
        "This snippet uses an augmented version of the original dataset_tool.py from Nvidia's repo that can handle a wider variety of image types without failing.\n",
        "\n",
        "### **Mac Users**\n",
        "\n",
        "The .zip folders you make on you Mac are likely to _fail_ if you use the standard method of compressing your folders (i.e. right-clicking on the folder and selecting _Compress \"folder\"_). Your Mac automatically creates a hidden folder in the .zip that becomes visible when you extract the .zip files on a non-Mac (like in this Colab, for instance). This causes problems.\n",
        "\n",
        "![bad zip](https://raw.githubusercontent.com/dougrosman/stylegan2-ada-pytorch/main/images/bad_zip_apple.png)\n",
        "\n",
        "To solve this, download and install a third-party app called [Keka](https://www.keka.io/en/), and use that to create a .zip folder.\n",
        "#### **To compress a folder with Keka:**\n",
        "1. Open Keka\n",
        "2. Check the box next to '**Exclude Mac resource forks**'\n",
        "3. Select '**ZIP**' from the top-right dropdown.\n",
        "4. Drag and drop your folder directly onto Keka\n",
        "5. It may ask you where to save your file. Select your destination and compress your folder. That .zip is safe to be uploaded to this Colab.\n",
        "\n",
        "![good zip with Keka](https://raw.githubusercontent.com/dougrosman/stylegan2-ada-pytorch/main/images/keka.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYi9zJK8FkC2"
      },
      "source": [
        "# Batch cropping and resizing your images\n",
        "\n",
        "If your the images in your .zip folder are not uniformly square, run this cell to automatically resize and center crop your images to the specified resolution. Set the following arguments in the cell below (you'll need to scroll all the way to the right to change the width/height). After setting them, run the cell. This will take a couple minutes, depending on the quantity of images in your data set. **If this process fails for any reason, please let Doug know.**\n",
        "\n",
        "* __`--width=\"1024\"`__ - set this to 1024 (or 512, or 256, depending on your needs)\n",
        "* __`--height=\"1024\"`__ - set this to the same value as your --width\n",
        "* __`--source=/path/to/your/dataset.zip`__ (just change the dataset name at the end of the path to the name of the data set you want to process)\n",
        "* __`--dest=/path/to/your/dataset-processed.zip`__ (same as above, but add \"-processed\" to the end of the filename (e.g. mydata-processed.zip)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv8x-0D-C0uF",
        "outputId": "f4ce36bf-d1b4-4443-9767-69b06e066e49"
      },
      "source": [
        "!python dataset_tool.py --source=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/datasets/banana.zip --dest=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/datasets/banana-processed.zip --width=1024 --height=1024 --transform=\"center-crop\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% 98/98 [00:16<00:00,  5.79it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B-h6FpB9FaK"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNc-3wTO-MUd"
      },
      "source": [
        "Below are a series of variables you need to set to run the training. You probably won’t need to touch most of them.\n",
        "\n",
        "* `dataset_path`: this is the path to your .zip file (make sure that if you ran the data processing step above, you point to the processed data set.)\n",
        "* `resume_from`: if you’re starting a new dataset I recommend `'ffhq1024'` or `'./pretrained/wikiart.pkl'` (You can download other pretrained models [here](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/). (for 512x512, you can use afhqcat.pkl, afhqdog.pkl,  afhqwild.pkl, or brecahad.pkl)\n",
        "* `mirror_x` and `mirror_y`: Allow the dataset to use horizontal or vertical mirroring.\n",
        "* `snapshot_count`: this determines how frequently your progress will be saved. If set to '1', a .pkl file will be saved with each tick (every ~30 mins or so). **I recommend this.** This will end up using a lot of space in your Google Drive, since you'll be saving a lot pickles, but if you have enough space, it's worth it. Setting this to '2' is also reasonable. This way, you are less likely to lose hours of training progress when Google shuts down your training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV0W6yxP-UIn"
      },
      "source": [
        "#required: definitely edit these!\n",
        "dataset_path = './datasets/apple-processed.zip'\n",
        "# resume_from = './pretrained/wikiart.pkl'\n",
        "resume_from = '/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00000-apple-processed-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000020.pkl'\n",
        "aug_strength = 0.173\n",
        "train_count = 20\n",
        "mirror_x = True\n",
        "\n",
        "#broken, don't use for now :(\n",
        "#mirror_y = False\n",
        "\n",
        "#optional: you might not need to edit these\n",
        "gamma_value = 50.0\n",
        "augs = 'bg'\n",
        "config = '11gb-gpu'\n",
        "snapshot_count = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL-M7WnnfMDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4371c5fb-f2b2-4108-9776-dc4b025eb2c2"
      },
      "source": [
        "!python train.py --gpus=1 --cfg=$config --metrics=None --outdir=./results --data=$dataset_path --snap=$snapshot_count --resume=$resume_from --augpipe=$augs --initstrength=$aug_strength --gamma=$gamma_value --mirror=$mirror_x --mirrory=False --nkimg=$train_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 1,\n",
            "  \"network_snapshot_ticks\": 1,\n",
            "  \"metrics\": [],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"./datasets/apple-processed.zip\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 100,\n",
            "    \"xflip\": true,\n",
            "    \"resolution\": 1024\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 8\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 32768,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 32768,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.002,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.002,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 50.0\n",
            "  },\n",
            "  \"total_kimg\": 25000,\n",
            "  \"batch_size\": 4,\n",
            "  \"batch_gpu\": 4,\n",
            "  \"ema_kimg\": 10,\n",
            "  \"ema_rampup\": null,\n",
            "  \"nimg\": 20000,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_p\": 0.173,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1\n",
            "  },\n",
            "  \"resume_pkl\": \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00000-apple-processed-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000020.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"run_dir\": \"./results/00002-apple-processed-mirror-11gb-gpu-gamma50-bg-resumecustom\"\n",
            "}\n",
            "\n",
            "Output directory:   ./results/00002-apple-processed-mirror-11gb-gpu-gamma50-bg-resumecustom\n",
            "Training data:      ./datasets/apple-processed.zip\n",
            "Training duration:  25000 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   100\n",
            "Image resolution:   1024\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    True\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "\n",
            "Num images:  200\n",
            "Image shape: [3, 1024, 1024]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Resuming from \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00000-apple-processed-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000020.pkl\"\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator              Parameters  Buffers  Output shape         Datatype\n",
            "---                    ---         ---      ---                  ---     \n",
            "mapping.fc0            262656      -        [4, 512]             float32 \n",
            "mapping.fc1            262656      -        [4, 512]             float32 \n",
            "mapping.fc2            262656      -        [4, 512]             float32 \n",
            "mapping.fc3            262656      -        [4, 512]             float32 \n",
            "mapping.fc4            262656      -        [4, 512]             float32 \n",
            "mapping.fc5            262656      -        [4, 512]             float32 \n",
            "mapping.fc6            262656      -        [4, 512]             float32 \n",
            "mapping.fc7            262656      -        [4, 512]             float32 \n",
            "mapping                -           512      [4, 18, 512]         float32 \n",
            "synthesis.b4.conv1     2622465     32       [4, 512, 4, 4]       float32 \n",
            "synthesis.b4.torgb     264195      -        [4, 3, 4, 4]         float32 \n",
            "synthesis.b4:0         8192        16       [4, 512, 4, 4]       float32 \n",
            "synthesis.b4:1         -           -        [4, 512, 4, 4]       float32 \n",
            "synthesis.b8.conv0     2622465     80       [4, 512, 8, 8]       float32 \n",
            "synthesis.b8.conv1     2622465     80       [4, 512, 8, 8]       float32 \n",
            "synthesis.b8.torgb     264195      -        [4, 3, 8, 8]         float32 \n",
            "synthesis.b8:0         -           16       [4, 512, 8, 8]       float32 \n",
            "synthesis.b8:1         -           -        [4, 512, 8, 8]       float32 \n",
            "synthesis.b16.conv0    2622465     272      [4, 512, 16, 16]     float32 \n",
            "synthesis.b16.conv1    2622465     272      [4, 512, 16, 16]     float32 \n",
            "synthesis.b16.torgb    264195      -        [4, 3, 16, 16]       float32 \n",
            "synthesis.b16:0        -           16       [4, 512, 16, 16]     float32 \n",
            "synthesis.b16:1        -           -        [4, 512, 16, 16]     float32 \n",
            "synthesis.b32.conv0    2622465     1040     [4, 512, 32, 32]     float32 \n",
            "synthesis.b32.conv1    2622465     1040     [4, 512, 32, 32]     float32 \n",
            "synthesis.b32.torgb    264195      -        [4, 3, 32, 32]       float32 \n",
            "synthesis.b32:0        -           16       [4, 512, 32, 32]     float32 \n",
            "synthesis.b32:1        -           -        [4, 512, 32, 32]     float32 \n",
            "synthesis.b64.conv0    2622465     4112     [4, 512, 64, 64]     float32 \n",
            "synthesis.b64.conv1    2622465     4112     [4, 512, 64, 64]     float32 \n",
            "synthesis.b64.torgb    264195      -        [4, 3, 64, 64]       float32 \n",
            "synthesis.b64:0        -           16       [4, 512, 64, 64]     float32 \n",
            "synthesis.b64:1        -           -        [4, 512, 64, 64]     float32 \n",
            "synthesis.b128.conv0   1442561     16400    [4, 256, 128, 128]   float16 \n",
            "synthesis.b128.conv1   721409      16400    [4, 256, 128, 128]   float16 \n",
            "synthesis.b128.torgb   132099      -        [4, 3, 128, 128]     float16 \n",
            "synthesis.b128:0       -           16       [4, 256, 128, 128]   float16 \n",
            "synthesis.b128:1       -           -        [4, 256, 128, 128]   float32 \n",
            "synthesis.b256.conv0   426369      65552    [4, 128, 256, 256]   float16 \n",
            "synthesis.b256.conv1   213249      65552    [4, 128, 256, 256]   float16 \n",
            "synthesis.b256.torgb   66051       -        [4, 3, 256, 256]     float16 \n",
            "synthesis.b256:0       -           16       [4, 128, 256, 256]   float16 \n",
            "synthesis.b256:1       -           -        [4, 128, 256, 256]   float32 \n",
            "synthesis.b512.conv0   139457      262160   [4, 64, 512, 512]    float16 \n",
            "synthesis.b512.conv1   69761       262160   [4, 64, 512, 512]    float16 \n",
            "synthesis.b512.torgb   33027       -        [4, 3, 512, 512]     float16 \n",
            "synthesis.b512:0       -           16       [4, 64, 512, 512]    float16 \n",
            "synthesis.b512:1       -           -        [4, 64, 512, 512]    float32 \n",
            "synthesis.b1024.conv0  51297       1048592  [4, 32, 1024, 1024]  float16 \n",
            "synthesis.b1024.conv1  25665       1048592  [4, 32, 1024, 1024]  float16 \n",
            "synthesis.b1024.torgb  16515       -        [4, 3, 1024, 1024]   float16 \n",
            "synthesis.b1024:0      -           16       [4, 32, 1024, 1024]  float16 \n",
            "synthesis.b1024:1      -           -        [4, 32, 1024, 1024]  float32 \n",
            "---                    ---         ---      ---                  ---     \n",
            "Total                  30370060    2797104  -                    -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape         Datatype\n",
            "---            ---         ---      ---                  ---     \n",
            "b1024.fromrgb  128         16       [4, 32, 1024, 1024]  float16 \n",
            "b1024.skip     2048        16       [4, 64, 512, 512]    float16 \n",
            "b1024.conv0    9248        16       [4, 32, 1024, 1024]  float16 \n",
            "b1024.conv1    18496       16       [4, 64, 512, 512]    float16 \n",
            "b1024          -           16       [4, 64, 512, 512]    float16 \n",
            "b512.skip      8192        16       [4, 128, 256, 256]   float16 \n",
            "b512.conv0     36928       16       [4, 64, 512, 512]    float16 \n",
            "b512.conv1     73856       16       [4, 128, 256, 256]   float16 \n",
            "b512           -           16       [4, 128, 256, 256]   float16 \n",
            "b256.skip      32768       16       [4, 256, 128, 128]   float16 \n",
            "b256.conv0     147584      16       [4, 128, 256, 256]   float16 \n",
            "b256.conv1     295168      16       [4, 256, 128, 128]   float16 \n",
            "b256           -           16       [4, 256, 128, 128]   float16 \n",
            "b128.skip      131072      16       [4, 512, 64, 64]     float16 \n",
            "b128.conv0     590080      16       [4, 256, 128, 128]   float16 \n",
            "b128.conv1     1180160     16       [4, 512, 64, 64]     float16 \n",
            "b128           -           16       [4, 512, 64, 64]     float16 \n",
            "b64.skip       262144      16       [4, 512, 32, 32]     float32 \n",
            "b64.conv0      2359808     16       [4, 512, 64, 64]     float32 \n",
            "b64.conv1      2359808     16       [4, 512, 32, 32]     float32 \n",
            "b64            -           16       [4, 512, 32, 32]     float32 \n",
            "b32.skip       262144      16       [4, 512, 16, 16]     float32 \n",
            "b32.conv0      2359808     16       [4, 512, 32, 32]     float32 \n",
            "b32.conv1      2359808     16       [4, 512, 16, 16]     float32 \n",
            "b32            -           16       [4, 512, 16, 16]     float32 \n",
            "b16.skip       262144      16       [4, 512, 8, 8]       float32 \n",
            "b16.conv0      2359808     16       [4, 512, 16, 16]     float32 \n",
            "b16.conv1      2359808     16       [4, 512, 8, 8]       float32 \n",
            "b16            -           16       [4, 512, 8, 8]       float32 \n",
            "b8.skip        262144      16       [4, 512, 4, 4]       float32 \n",
            "b8.conv0       2359808     16       [4, 512, 8, 8]       float32 \n",
            "b8.conv1       2359808     16       [4, 512, 4, 4]       float32 \n",
            "b8             -           16       [4, 512, 4, 4]       float32 \n",
            "b4.mbstd       -           -        [4, 513, 4, 4]       float32 \n",
            "b4.conv        2364416     16       [4, 512, 4, 4]       float32 \n",
            "b4.fc          4194816     -        [4, 512]             float32 \n",
            "b4.out         513         -        [4, 1]               float32 \n",
            "---            ---         ---      ---                  ---     \n",
            "Total          29012513    544      -                    -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "2021-03-30 16:54:05.978018: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Training for 25000 kimg...\n",
            "\n",
            "tick 0     kimg 20.0     time 1m 26s       sec/tick 15.6    sec/kimg 3896.78 maintenance 70.7   cpumem 4.74   gpumem 12.93  augment 0.173\n",
            "tick 1     kimg 24.0     time 32m 10s      sec/tick 1839.6  sec/kimg 459.90  maintenance 4.5    cpumem 4.81   gpumem 7.72   augment 0.206\n",
            "\n",
            "Aborted!\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgvSvfyi_R_-"
      },
      "source": [
        "### Resume Training\n",
        "\n",
        "Once Colab has shutdown, you’ll need to resume your training. Reset the variables above, particularly the `resume_from` and `aug_strength` settings.\n",
        "\n",
        "1. Point `resume_from` to the last .pkl you trained (you’ll find these in the `results` folder)\n",
        "2. Update `aug_strength` to match the augment value of the last pkl file. Often you’ll see this in the console, but you may need to look at the `log.txt`. Updating this makes sure training stays as stable as possible.\n",
        "3. You may want to update `train_count` to keep track of your training progress.\n",
        "\n",
        "Once all of this has been reset, run that variable cell and the training command cell after it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VznRirOE5ENI"
      },
      "source": [
        "## Convert Legacy Model\n",
        "\n",
        "If you have an older version of a model (Tensorflow based StyleGAN, or Runway downloaded .pkl file) you’ll need to convert to the newest version. If you’ve trained in this notebook you do **not** need to use this cell.\n",
        "\n",
        "`--source`: path to model that you want to convert\n",
        "\n",
        "`--dest`: path and file name to convert to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzkP-Rww5Np9"
      },
      "source": [
        "!python legacy.py --source=/content/drive/MyDrive/runway.pkl --dest=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/runway.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6EtrPqL9ILk"
      },
      "source": [
        "## Testing/Inference\n",
        "\n",
        "Also known as \"Inference\", \"Evaluation\" or \"Testing\" the model. This is the process of usinng your trained model to generate new material, usually images or videos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYdyfH0O8In_"
      },
      "source": [
        "### Generate Single Images\n",
        "\n",
        "`--network`: Make sure the `--network` argument points to your .pkl file. (My preferred method is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that into the argument after the `=` sign).\n",
        "\n",
        "`--seeds`: This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation.\n",
        "\n",
        "`--truncation`: Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. The smaller the number the more realistic your images should appear, but this will also affect diversity. Most people choose between 0.5 and 1.0, but technically it's infinite. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYRXenMoZSHf"
      },
      "source": [
        "!python generate.py --outdir=/content/out/images/ --trunc=1.0 --seeds=0-49 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjOTCWVonoVL"
      },
      "source": [
        "### Truncation Traversal\n",
        "\n",
        "Below you can take one seed and look at the changes to it across any truncation amount. -1 to 1 will be pretty realistic images, but the further out you get the weirder it gets.\n",
        "\n",
        "#### Options \n",
        "`--network`: Again, this should be the path to your .pkl file.\n",
        "\n",
        "`--seeds`: Pass this only one seed. Pick a favorite from your generated images.\n",
        "\n",
        "`--start`: Starting truncation value.\n",
        "\n",
        "`--stop`: Stopping truncation value. This should be larger than the start value. (Will probably break if its not).\n",
        "\n",
        "`--increment`: How much each frame should increment the truncation value. Make this really small if you want a long, slow interpolation. (stop-start/increment=total frames)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyzdGr7OnrMG"
      },
      "source": [
        "!python generate.py --process=\"truncation\" --outdir=/content/out/trunc-trav-3/ --start=-0.8 --stop=2.8 --increment=0.02 --seeds=470 --network=/content/drive/MyDrive/stylegan2-transfer-models/mixed6k-network-snapshot-016470.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSzj0igO8Lfu"
      },
      "source": [
        "### Interpolations\n",
        "\n",
        "Interpolation is the process of generating very small changes to a vector in order to make it appear animated from frame to frame.\n",
        "\n",
        "We’ll look at different examples of interpolation below.\n",
        "\n",
        "#### Options\n",
        "\n",
        "`--network`: path to your .pkl file\n",
        "\n",
        "`--interpolation`: Walk type defines the type of interpolation you want. In some cases it can also specify whether you want the z space or the w space.\n",
        "\n",
        "`--frames`: How many frames you want to produce. Use this to manage the length of your video.\n",
        "\n",
        "`--trunc`: truncation value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJSqafIzNwhx"
      },
      "source": [
        "#### Linear Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqkiskly8S5_"
      },
      "source": [
        "!python generate.py --outdir=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/generated/fruit_veg1024-2 --space=\"z\" --trunc=0.75 --process=\"interpolation\" --seeds=1,2,4,5,6,1 --network=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00000-fruit_veg1024-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000016.pkl --frames=120 --fps=60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCUEV3aO8s_X"
      },
      "source": [
        "!python generate.py --outdir=out/video1-w/ --space=\"w\" --trunc=1 --process=\"interpolation\" --seeds=85,265,297,849 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmKbwZDD8gjM"
      },
      "source": [
        "!zip -r vid1.zip /content/out/video1-w-0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi3d7xzpN2Uj"
      },
      "source": [
        "#### Slerp Interpolation\n",
        "\n",
        "This gets a little heady, but technically linear interpolations are not the best in high-dimensional GANs. [This github link](https://github.com/soumith/dcgan.torch/issues/14) is one of the more popular explanations ad discussions.\n",
        "\n",
        "In reality I do not find a huge difference between linear and spherical interpolations (the difference in z- and w-space is enough in many cases), but I’ve implemented slerp here for anyone interested.\n",
        "\n",
        "Note: Slerp in w space currently isn’t supported. I’m working on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0-cUd3fB_kJ"
      },
      "source": [
        "!python generate.py --outdir=out/video1/ --trunc=1 --process=\"interpolation\" --interpolation=\"slerp\" --seeds=85,265,297,849 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP1HsU_CPcF5"
      },
      "source": [
        "#### Noise Loop\n",
        "\n",
        "If you want to just make a random but fun interpolation of your model the noise loop is the way to go. It creates a random path thru the z space to show you a diverse set of images.\n",
        "\n",
        "`--interpolation=\"noiseloop\"`: set this to use the noise loop funtion\n",
        "\n",
        "`--diameter`: This controls how \"wide\" the loop is. Make it smaller to show a less diverse range of samples. Make it larger to cover a lot of samples. This plus `--frames` can help determine how fast the video feels.\n",
        "\n",
        "`--random_seed`: this allows you to change your starting place in the z space. Note: this value has nothing to do with the seeds you use to generate images. It just allows you to randomize your start point (and if you want to return to it you can use the same seed multiple times).\n",
        "\n",
        "Noise loops currently only work in z space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfR6DhfvN8b_"
      },
      "source": [
        "!python generate.py --outdir=out/video-noiseloop-0.9d/ --trunc=0.8 --process=\"interpolation\" --interpolation=\"noiseloop\" --diameter=0.9 --random_seed=100 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkKFb-4CedOq"
      },
      "source": [
        "#### Circular Loop\n",
        "\n",
        "The noise loop is, well, noisy. This circular loop will feel much more even, while still providing a random loop.\n",
        "\n",
        "I recommend using a higher `--diameter` value than you do with noise loops. Something between `50.0` and `500.0` alongside `--frames` can help control speed and diversity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao62za9_QfOF"
      },
      "source": [
        "!python generate.py --outdir=out/video-circularloop/ --trunc=1 --process=\"interpolation\" --interpolation=\"circularloop\" --diameter=800.00 --frames=720 --random_seed=90 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Ld_ozNJCOn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qywlaS5pgzyH"
      },
      "source": [
        "## Combine NPZ files together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2VooqrNfIpw"
      },
      "source": [
        "!python combine_npz.py --outdir=/content/npz --npzs='/content/pb-proj1-3-clip.npz,/content/pb-proj1-clip-nocenter.npz,/content/pb-proj2-clip-nocenter.npz,/content/projected_w (2).npz'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}