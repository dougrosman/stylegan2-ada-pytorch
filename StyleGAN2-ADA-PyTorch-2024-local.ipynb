{"cells":[{"cell_type":"markdown","metadata":{"id":"jG7ZEc_982io"},"source":["# StyleGAN2-ADA-PyTorch-DR\n","\n","## Preface\n","This StyleGAN2-ADA-PyTorch repository (including this Colab notebook) was forked from [Derrick Schultz](https://github.com/dvschultz/stylegan2-ada-pytorch), which was forked from [Nvidia's original repo](https://github.com/NVlabs/stylegan2-ada-pytorch). Additional comments and instructions added by Doug Rosman.\n","\n","Many thanks to Derrick for developing these tools to make GANs accessible for artists and newcomers, and for generously sharing his knowledge and resources **for free**. Without artists/educators like this, we would be spending much more time (and money) getting these things working. If you find that you're using this Colab notebook extensively, and especially if you're using it to make work to display outside the classroom, **_please be sure to credit [Derrick Schultz](https://www.instagram.com/dvsmethid/?hl=en)_**, and I encourage you all to consider signing up for his [Patreon](https://www.patreon.com/bustbright) or [YouTube channel](https://www.youtube.com/channel/UCaZuPdmZ380SFUMKHVsv_AA/join). You can also send him a one-time payment on [Venmo](https://venmo.com/Derrick-Schultz). Derrick also has a Slack channel dedicated to this stuff, which you can access as a Patreon subscriber (it's a good place to get some extra help).\n","\n","~ ~ ~ ~ ~ ~ ~ ~ ~\n","\n","This Notebook was last updated: March 4, 2023 by [Doug Rosman](https://dougrosman.com/).\n","\n","Additional Colab and Python help from Blake Fall-Conroy.\n","\n","~ ~ ~ ~ ~ ~ ~ ~ ~"]},{"cell_type":"markdown","metadata":{"id":"nTxKm73p6Vv1"},"source":["# Part 1: Setup (required every time you run this notebook)\n"]},{"cell_type":"markdown","metadata":{"id":"wgBoxVzLUjz7"},"source":["These setup steps are required to get your GPU instance running, mount your Google Drive, and install the necessary dependencies."]},{"cell_type":"markdown","metadata":{"id":"NetPDYQl-CSx"},"source":["## 1.1 Connect to a GPU Instance (required if training or generating images)"]},{"cell_type":"markdown","metadata":{"id":"NaMTJ1O9-PrN"},"source":["**SKIP THIS STEP IF YOU ARE DOING NON-GPU RELATED TASKS**\n","This command will connect you to a GPU instance and randomly select a GPU. The GPU you get depends on the subscription tier you have.\n","\n","- **A100** - amazing, Pro+ only, train/generate up to 1024x1024\n","- **V100** - great, Pro+ only, train/generate up to 1024x1024\n","- **P100** - solid, Pro+ only, train/generate up to 1024x1024\n","- **T4** - good, Free and Pro, train up to 512x512, generate up to 1024x1024\n","- **K80** - okay, Free, train* up to 512x512, generate up to 1024x1024\n","\n","If you get a K80 when you're trying to train a model, I recommend terminating the session and starting again (hopefully you'll get a better GPU!). **To terminate your session, at the top of your screen go to Runtime --> Manage Sessions --> Terminate**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":590,"status":"ok","timestamp":1678219889121,"user":{"displayName":"Doug Rosman","userId":"12469472491464808109"},"user_tz":360},"id":"SO_YKcaT-NBq","outputId":"bae22ae9-fa99-49c5-b2c4-83317052125e"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU 0: NVIDIA GeForce RTX 4090 (UUID: GPU-4cc36cd3-6aa8-06a2-ec0b-dee66b991388)\n"]}],"source":["!nvidia-smi -L"]},{"cell_type":"markdown","metadata":{"id":"-9DH84cXALLF"},"source":["## 1.3 Initialize the StyleGAN2-ADA-PyTorch repository (required)"]},{"cell_type":"markdown","metadata":{"id":"O-Q2isKRBRnn"},"source":["This clones my fork of the StyleGAN2-ada-PyTorch GitHub repository to your Google Drive. If the repository already exists, this command will `cd` (change directory) into the correct folder."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"B8ADVNpBh8Ox"},"outputs":[{"name":"stderr","output_type":"stream","text":["The syntax of the command is incorrect.\n"]}],"source":["import os\n","\n","!mkdir downloads\n","!mkdir datasets\n","!mkdir pretrained\n","!mkdir generated\n","!mkdir generated/projection"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VBRt41MUBFbE"},"source":["## 1.5 Install Dependencies (required if training or generating images)"]},{"cell_type":"markdown","metadata":{"id":"SLNDnMd6B1ro"},"source":["This installs the required dependencies, and uninstalls some libraries that create conflicts"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"n350xrkcA2vf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.9.0+cu111\n","  Using cached https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp39-cp39-win_amd64.whl (3128.0 MB)\n","Collecting torchvision==0.10.0+cu111\n","  Using cached https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp39-cp39-win_amd64.whl (2.5 MB)\n","Requirement already satisfied: typing-extensions in c:\\users\\doug\\documents\\github\\stylegan2-ada-pytorch\\.conda\\lib\\site-packages (from torch==1.9.0+cu111) (4.10.0)\n","Collecting numpy (from torchvision==0.10.0+cu111)\n","  Using cached numpy-1.26.4-cp39-cp39-win_amd64.whl.metadata (61 kB)\n","Collecting pillow>=5.3.0 (from torchvision==0.10.0+cu111)\n","  Using cached pillow-10.2.0-cp39-cp39-win_amd64.whl.metadata (9.9 kB)\n","Using cached pillow-10.2.0-cp39-cp39-win_amd64.whl (2.6 MB)\n","Using cached numpy-1.26.4-cp39-cp39-win_amd64.whl (15.8 MB)\n","Installing collected packages: torch, pillow, numpy, torchvision\n","Successfully installed numpy-1.26.4 pillow-10.2.0 torch-1.9.0+cu111 torchvision-0.10.0+cu111\n","Collecting timm==0.4.12\n","  Using cached timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n","Collecting ftfy==6.1.1\n","  Using cached ftfy-6.1.1-py3-none-any.whl.metadata (6.1 kB)\n","Collecting ninja==1.10.2\n","  Using cached ninja-1.10.2-py2.py3-none-win_amd64.whl.metadata (5.0 kB)\n","Collecting opensimplex\n","  Using cached opensimplex-0.4.5-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: torch>=1.4 in c:\\users\\doug\\documents\\github\\stylegan2-ada-pytorch\\.conda\\lib\\site-packages (from timm==0.4.12) (1.9.0+cu111)\n","Requirement already satisfied: torchvision in c:\\users\\doug\\documents\\github\\stylegan2-ada-pytorch\\.conda\\lib\\site-packages (from timm==0.4.12) (0.10.0+cu111)\n","Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\doug\\documents\\github\\stylegan2-ada-pytorch\\.conda\\lib\\site-packages (from ftfy==6.1.1) (0.2.13)\n","Requirement already satisfied: numpy>=1.22 in c:\\users\\doug\\documents\\github\\stylegan2-ada-pytorch\\.conda\\lib\\site-packages (from opensimplex) (1.26.4)\n","Requirement already satisfied: typing-extensions in c:\\users\\doug\\documents\\github\\stylegan2-ada-pytorch\\.conda\\lib\\site-packages (from torch>=1.4->timm==0.4.12) (4.10.0)\n","Requirement already satisfied: pillow>=5.3.0 in c:\\users\\doug\\documents\\github\\stylegan2-ada-pytorch\\.conda\\lib\\site-packages (from torchvision->timm==0.4.12) (10.2.0)\n","Using cached timm-0.4.12-py3-none-any.whl (376 kB)\n","Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n","Using cached ninja-1.10.2-py2.py3-none-win_amd64.whl (837 kB)\n","Using cached opensimplex-0.4.5-py3-none-any.whl (268 kB)\n","Installing collected packages: ninja, opensimplex, ftfy, timm\n","Successfully installed ftfy-6.1.1 ninja-1.10.2 opensimplex-0.4.5 timm-0.4.12\n","Collecting tqdm\n","  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n","Requirement already satisfied: colorama in c:\\users\\doug\\documents\\github\\stylegan2-ada-pytorch\\.conda\\lib\\site-packages (from tqdm) (0.4.6)\n","Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n","Installing collected packages: tqdm\n","Successfully installed tqdm-4.66.2\n","Collecting imageio-ffmpeg\n","  Using cached imageio_ffmpeg-0.4.9-py3-none-win_amd64.whl.metadata (1.7 kB)\n","Requirement already satisfied: setuptools in c:\\users\\doug\\documents\\github\\stylegan2-ada-pytorch\\.conda\\lib\\site-packages (from imageio-ffmpeg) (68.2.2)\n","Using cached imageio_ffmpeg-0.4.9-py3-none-win_amd64.whl (22.6 MB)\n","Installing collected packages: imageio-ffmpeg\n","Successfully installed imageio-ffmpeg-0.4.9\n","Collecting click\n","  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: colorama in c:\\users\\doug\\documents\\github\\stylegan2-ada-pytorch\\.conda\\lib\\site-packages (from click) (0.4.6)\n","Using cached click-8.1.7-py3-none-any.whl (97 kB)\n","Installing collected packages: click\n","Successfully installed click-8.1.7\n","Collecting requests\n","  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n","Collecting charset-normalizer<4,>=2 (from requests)\n","  Using cached charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl.metadata (34 kB)\n","Collecting idna<4,>=2.5 (from requests)\n","  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n","Collecting urllib3<3,>=1.21.1 (from requests)\n","  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n","Collecting certifi>=2017.4.17 (from requests)\n","  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n","Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n","Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n","Using cached charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl (100 kB)\n","Using cached idna-3.6-py3-none-any.whl (61 kB)\n","Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n","Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n","Successfully installed certifi-2024.2.2 charset-normalizer-3.3.2 idna-3.6 requests-2.31.0 urllib3-2.2.1\n","Collecting scipy\n","  Using cached scipy-1.12.0-cp39-cp39-win_amd64.whl.metadata (60 kB)\n","Requirement already satisfied: numpy<1.29.0,>=1.22.4 in c:\\users\\doug\\documents\\github\\stylegan2-ada-pytorch\\.conda\\lib\\site-packages (from scipy) (1.26.4)\n","Using cached scipy-1.12.0-cp39-cp39-win_amd64.whl (46.2 MB)\n","Installing collected packages: scipy\n","Successfully installed scipy-1.12.0\n","Requirement already satisfied: psutil in c:\\users\\doug\\documents\\github\\stylegan2-ada-pytorch\\.conda\\lib\\site-packages (5.9.8)\n"]}],"source":["#GPU frontend\n","# !pip install \"jax[cuda11_cudnn805]==0.3.10\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n","\n","\n","#Downgrade Pytorch\n","!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install timm==0.4.12 ftfy==6.1.1 ninja==1.10.2 opensimplex\n","!pip install tqdm\n","!pip install imageio-ffmpeg\n","!pip install click\n","!pip install requests\n","!pip install scipy\n","!pip install psutil"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mi5r1m4uR-xf"},"source":["## 1.6 Finding pretrained models"]},{"cell_type":"markdown","metadata":{"id":"PqgtDzuuSD8t"},"source":["Training models from scratch takes a lot of time, so it's best to transfer learn from a pretrained model.\n","\n","Download any of the models below, and then upload the .pkl file to the `pretrained` folder in the stylegan2-ada-pytorch folder in your Google Drive. Note: once you train your own models, you can use those as pretrained models to transfer learn from.\n","\n","**Important considerations for picking a pretrained model**\n","1. The pretrained model must be the same dimensions as your data set (e.g., if your images are 512x512, your trained model must also be 512x512)\n","2. The pretrained model's config must match your training config. **Unless you know what you're doing, stick with models labeled config-f.**\n","\n","**Lists of pretrained models to choose from**\n","- https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ (Nvidia's official models (ada version))\n","  - [FFHQ](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl) (1024) (Flickr Faces High Quality, realistic human faces)\n","  - [MetFaces](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl) (1024) (Faces from paintings in the Met Museum of Art Collection)\n","  - [AFHQ Wild](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqwild.pkl) (512) (Animal Faces High Quality, realistic wild animal faces)\n","  - [AFHQ Cat](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqcat.pkl) (512) (Animal Faces High Quality, realistic cat faces)\n","  - [AFHQ Dog](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqdog.pkl) (512) (Animal Faces High Quality, realistic dog faces)\n","  - [BreCaHAD](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/brecahad.pkl) (512) (Breast Cancer microscopic biopsy images)\n","\n","- https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/ (Nvidia's official models)\n","  - [Cat](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl) (256)\n","  - [Church](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-church-config-f.pkl) (256)\n","  - [Horse](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-horse-config-f.pkl) (256)\n","\n","- https://github.com/justinpinkney/awesome-pretrained-stylegan2 (collection by Justin Pinkney. There are more there than the few listed below)\n","  - [FFHQ 512](https://mega.nz/#!eQdHkShY!8wyNKs343L7YUjwXlEg3cWjqK2g2EAIdYz5xbkPy3ng) (512) (Lower resolution version of the FFHQ model)\n","  - [Flowers](https://drive.google.com/uc?id=13onBTt6xVwKmYTRCFbONBTbSXFqeiVcK) (256)\n","  - [Anime Portraits](https://mega.nz/#!PeIi2ayb!xoRtjTXyXuvgDxSsSMn-cOh-Zux9493zqdxwVMaAzp4) (512)"]},{"cell_type":"markdown","metadata":{"id":"bqsU3uta6jzo"},"source":["# Part 2: Data Set Preparation"]},{"cell_type":"markdown","metadata":{"id":"Mj7LEr6LUeDK"},"source":["Prepare your images for training by uploading a zip folder of images, then resizing and cropping them to your desired dimensions. This notebook contains a script for batch resizing and cropping your images. That said, you should take time to refine and clean your data set before uploading it to use in this notebook."]},{"cell_type":"markdown","metadata":{"id":"OpHg9Y9DEOrC"},"source":["## 2.1 Zipping up your images and uploading to Google Drive\n"]},{"cell_type":"markdown","metadata":{"id":"OdncQC9FInep"},"source":["Create a folder and name it based on your data set (don't put spaces in your filename), then put all your images into that folder\n","\n","### Creating a zip file\n","\n","#### Windows Users:\n","1. Right-click the folder and select **Send to** --> **Compressed (zipped) folder**\n","1. Upload the .zip file to the **datasets** folder in Google Drive (inside the colab-sg2-ada-pytorch folder in your Google Drive)\n","\n","#### Mac Users:\n","Zipping files on a Mac with the native compression tool creates a file called \".DS_Store\" inside the folder, which is annoying because it causes problems. These steps include a workaround.\n","\n","##### Download and Install [Keka](https://www.keka.io/en/)\n","1. Download Keka ([click to download](https://d.keka.io/))\n","1. Install Keka by dragging and dropping it into your Applications folder\n","1. Open Keka\n","1. Make sure the box next to **'Exclude Mac resource forks'** is checked\n","1. Drag and draop your data set folder directly onto the open Keka app\n","1. It may ask you where to save your file. Select your desintation and compress your folder\n","1. Upload the .zip file to the **datasets** folder in Google Drive (inside the colab-sg2-ada-pytorch folder in your Google Drive)\n","\n","![good zip with Keka](https://raw.githubusercontent.com/dougrosman/stylegan2-ada-pytorch/main/images/keka.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IB1t-nTSItRZ"},"source":["## 2.2 Processing your images (batch cropping + resizing)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mQc-JnFPmQLe"},"source":["In order to train, all your images must be the exact same dimensions, and must be RGB images. Whatever state your data set is in, this step will resize and auto-crop each image to ensure the training will work. If your data set is already cleaned up and ready to go, you should still do this step (it won't change anything that's already in the correct format).\n","\n","Taking care to prepare your data is an important part of working with these tools, so your data set *should* be ready to go before uploading.\n","\n","StyleGAN2 most easily supports the following square resolutions:\n","- 256x256\n","- 512x512\n","- 1024x1024\n","\n","This process processes each image with these steps:\n","1. Convert image to RGB\n","2. Upscale or downscale image as needed\n","3. Center-crop image"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ELqFOOtWB-P3"},"outputs":[],"source":["# Set your variables, then run this cell (required).\n","\n","width = 1024  # EDIT THIS to your desired dimension (256, 512 or 1024)\n","height = width\n","dataset = \"morning_test\" # EDIT THIS to the name of your data set file (without the .zip)\n","\n","data_src = \"./datasets/\" + dataset + \".zip\"\n","data_dest = \"./datasets/\" + dataset + \"-\" + str(width) + \".zip\" "]},{"cell_type":"code","execution_count":7,"metadata":{"id":"pFa0yeGaDkRO"},"outputs":[{"name":"stdout","output_type":"stream","text":["Error: Missing input file or directory: ./datasets/morning_test.zip\n"]}],"source":["# Run this to process your images\n","!python dataset_tool-png.py --source=$data_src --dest=$data_dest --width=$width --height=$height --transform=\"center-crop\""]},{"cell_type":"markdown","metadata":{"id":"5B-h6FpB9FaK"},"source":["# Part 3: Training your model"]},{"cell_type":"markdown","metadata":{"id":"ul5PPscSfIj2"},"source":["## 3.0 Training Tips/FAQ"]},{"cell_type":"markdown","metadata":{"id":"wEL26dTkd8op"},"source":["### Tips\n","1. The more images you have, the better your model, but the longer it will take to train\n","1. Training at smaller resolutions will be faster.\n","1. Consider training a low resolution (256x256) model on your data first to see if you like the results. If you do, start a new training at a higher resolution.\n","1. Your model might be \"good enough\" after a small amount of training (200 kimg or so), but training longer will refine the quality of your images. Aim for 1,000 kimgs if you can.\n","\n","### FAQ\n","\n","**How many images should I have in my data set?**\n","\n","It totally depends on your artistic goals! More images of your subject matter will lead to more \"accurate\" results, but that might not be what you're going for. Let's say that for \"accuracy\", you should have at least 500 images, but you should really be aiming for 2,000-5,000 (or more!). The FFHQ data set (Flickr Faces High Quality) that makes photorealistic faces was trained on 70,000 faces!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LCibYLXYPHPg"},"source":["## 3.1 Setting your variables"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"2EXUik2DJtrg"},"outputs":[],"source":["# Set the following variables to make sure you're training with the correct settings.\n","\n","# EDIT THIS to the name of your data set file (without the .zip)\n","dataset = \"morning_test\"\n","\n","# EDIT THIS to the size of your images (256, 512 or 1024)\n","size = 1024 \n","\n","# EDIT THIS to the .pkl file you're transfer learning from. This file must be in the \"pretrained\" folder\n","resume_from = \"metfaces.pkl\"\n","\n","# (OPTIONAL) EDIT THESE\n","snapshot_count = 2  # set how frequently you save checkpoints. Lower == more frequent\n","mirror_x = False # set to 'True' to double your data set size by duplicating and mirroring each image horizontally. \n","mirror_y = False # same as above, but vertically\n","\n","\n","# (DON'T CHANGE THESE UNLESS YOU KNOW WHAT YOU'RE DOING)\n","gamma_value = 50.0\n","augs = 'bg'\n","# config = '11gb-gpu'\n","config = '24gb-gpu'\n","\n","####\n","dataset_path = \"./datasets/\" + dataset + \"-\" + str(size) + \".zip\"\n","output_dir = \"./results\"\n","resume_from = \"./pretrained/\" + resume_from\n","aug_strength = 0\n","train_count = 0 "]},{"cell_type":"markdown","metadata":{"id":"aGAdtpujMs6s"},"source":["## 3.2 Setting your variables for resuming training (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"T-RZ9qbqadAs"},"source":["\n","Only edit and run this cell if you are resuming a training your already started. You must edit and run the cell in 3.1 first.\n","\n","* `resume_from`: The .pkl file you want to continue training from. This will probably be your latest .pkl file. You'll find this in your **results** folder.\n","* `aug_strength`: Set this to the augmentation amount for the pickle file you're resuming from. You can find this value in the \"log.txt\" file inside the results folder from the previous training. The number you're looking for will be in the right-most column of your log (**e.g. augment 0.652**). Select the augment value for the .pkl value you'll be training from. The number of kimgs will match the .pkl file.\n","* `train_count`: set this number to be the number at the end of the .pkl file you're resuming your training from. For example, if you're resuming your training from *network-snapshot-000200.pkl*, then set train_count to **200**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaOvn472MJk9"},"outputs":[],"source":["# ONLY DO THIS IF RESUMING FROM A TRAINING YOU'VE ALREADY STARTED\n","# Set the following arguments if you are resuming a training, then run this cell to save them\n","# EDIT THIS to the filepath of the .pkl you're resuming from\n","resume_from = './results/00001-your_dataset-512-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000020.pkl'\n","aug_strength = 0.003  # EDIT THIS TO YOUR CORRECT VALUE\n","train_count = 20  # EDIT THIS TO YOUR CORRECT VALUE"]},{"cell_type":"markdown","metadata":{"id":"Sb1s2hDhSOsf"},"source":["## 3.3 Start Training\n"]},{"cell_type":"markdown","metadata":{"id":"ZFHDPycXdOv0"},"source":["This is it, the moment we've been waiting for! Once you start your training, remember to keep this tab open and your computer on (make sure you computer doesn't fall asleep. If you have Pro+, I believe you can close the tab and training will still continue.)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"vm0zduxRLsvl"},"outputs":[],"source":["# After setting your variables above, run this cell to start your training.\n","!python train.py --gpus=1 --cfg=$config --metrics=None --outdir=$output_dir --data=$dataset_path --snap=$snapshot_count --resume=$resume_from --augpipe=$augs --initstrength=$aug_strength --gamma=$gamma_value --mirror=$mirror_x --mirrory=False --nkimg=$train_count"]},{"cell_type":"markdown","metadata":{"id":"tPpeQzPudaYC"},"source":["# Part 4: Generating Images and Videos"]},{"cell_type":"markdown","metadata":{"id":"WL2V-Teqoiei"},"source":["For an in-depth explanation of the latent space, interpolations, and the different ways to generate latent space walks, check out this video: [StyleGAN2 In-Depth Week 3 (latent spaces, linear interpolations and noise loops](https://www.youtube.com/watch?v=jKJCv9VGqLQ&t=1998s) by Artificial Images (Derrick Schultz). The link takes you to the part of the video where he begins to go through these. (33:18) "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 4.0 Key Terms"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vGvgZDjAlRmG"},"source":["- `network` - The pickle file to generate from. Network is another term for \"trained model.\"\n","- `seed` - Our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation. Can be a number between 0 to 4,294,967,295. \n","- `truncation` - How much to \"truncate\" the latent space. A measure of how \"creative\" or \"realistic\" the generator will be. Best results between -1 and 1. Low truncation (close to 0) produces a lower variety of images, adhering to the \"average\" image of your trained model. Higher truncation (close to 1) produces more varied images. Beyond -1 or 1, the results start getting more wonky, as these images are outside the bounds of the latent space.\n","- `interpolation` - Interpolation is a general concept of moving between two things. A trained model is a 512-dimension space, and *interpolating through* this space takes us through a range of images. There are different interpolation methods, each of which creates different \"routes\" through the latent space."]},{"cell_type":"markdown","metadata":{"id":"8zoiE_clkGsc"},"source":["## 4.1 Single random images"]},{"cell_type":"markdown","metadata":{"id":"flkupg4pkS79"},"source":["This generates individual images for each seed value. This can be useful for finding a good starting image for the interpolation loops."]},{"cell_type":"markdown","metadata":{"id":"mYdyfH0O8In_"},"source":["**Options**\n","\n","`--outdir`: Output directory (folder) where your images will be saved.\n","\n","`--trunc`: How much truncation, a number between -1 to 1 will produce the best results, but technically can be infinite. \n","\n","`--seeds`: The seeds to generate images from. Can use a range of values (e.g. 400-500), or comma-separated seed values (e.g. 10, 20, 30).\n","\n","`--network`: Make sure the `--network` argument points to your .pkl file. (My preferred method is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that into the argument after the `=` sign).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYRXenMoZSHf"},"outputs":[],"source":["!python generate.py \\\n","--outdir=./generated/random-1000-1020-noncon \\\n","--trunc=1 \\\n","--seeds=1000-1020 \\\n","--network=./pretrained/cat256.pkl"]},{"cell_type":"markdown","metadata":{"id":"VjOTCWVonoVL"},"source":["## 4.2 Truncation Traversal\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5PIuUSgiuPOm"},"source":["Below you can take one seed and look at the changes to it across any truncation amount. -1 to 1 will be pretty realistic images, but the further out you get the weirder it gets.\n","\n","**Options**\n","\n","`--outdir`: Output directory (folder) where your images will be saved.\n","\n","`--start`: Starting truncation value.\n","\n","`--stop`: Stopping truncation value. This should be larger than the start value. (Will probably break if its not).\n","\n","`--seeds`: Pass this only one seed. Pick a favorite from your generated images.\n","\n","`--increment`: How much each frame should increment the truncation value. Make this really small if you want a long, slow interpolation. (stop-start/increment=total frames)\n","\n","`--network`: Again, this should be the path to your .pkl file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nyzdGr7OnrMG"},"outputs":[],"source":["!python generate.py \\\n","--process=\"truncation\" \\\n","--outdir=./generated/trunc-1002_1004__-2-2 \\\n","--start=-2 \\\n","--stop=2 \\\n","--increment=0.005 \\\n","--seeds=1002 \\\n","--network=./pretrained/cat256.pkl \\\n","--fps=60"]},{"cell_type":"markdown","metadata":{"id":"OSzj0igO8Lfu"},"source":["## 4.3 Linear Interpolation (Lerp)\n"]},{"cell_type":"markdown","metadata":{"id":"FDLhM9Q8wUp2"},"source":["A straight line from one point (seed) in the latent space to another. The distance from one seed to another will vary, so some interpolations will be fast, others slow.\n","\n","**Options**\n","\n","`--outdir`: Output directory (folder) where your images will be saved.\n","\n","`--space`: \"z\" or \"w\". Depending on your model, w-space may interpolate more smoothly (like with faces).\n","\n","`--trunc`: truncation value\n","\n","`--seeds`: list of seeds to interpolate between. Comma separated. Each set of seeds will be given the same number of frames to complete the interpolation. Set the last seed to the same number as the first to create a seamless looping video.\n","\n","`--network`: path to your .pkl file\n","\n","`--frames`: How many frames to produce per interpolation. (e.g. if you set your frames to **30** and your seeds to 10, 20, 30, 10, this will create a video with **90** frames (10 to 20 (30 frames), 20 to 30 (30 frames), 30 to 10 (30 frames))\n","\n","`--fps`: frames per second of your video (24, 30, 48, 60, 120, 144)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqkiskly8S5_"},"outputs":[],"source":["!python generate.py \\\n","--outdir=./generated/lerp-10_20_30_40_10-w2 \\\n","--space=\"w\" \\\n","--trunc=0.75 \\\n","--process=\"interpolation\" \\\n","--seeds=10,20,30,40,10 \\\n","--network=./pretrained/cat256.pkl \\\n","--frames=120 \\\n","--fps=60"]},{"cell_type":"markdown","metadata":{"id":"uP1HsU_CPcF5"},"source":["## 4.4 Noise Loop\n"]},{"cell_type":"markdown","metadata":{"id":"loD7PzbyNqMK"},"source":["If you want to just make a random but fun interpolation of your model the noise loop is the way to go. It creates a random path through the z space to show you a diverse set of images.\n","\n","**Options**\n","\n","`--outdir`: Output directory (folder) where your images will be saved.\n","\n","`--trunc`: truncation value\n","\n","`--diameter`: This controls how \"wide\" the loop is. Make it smaller to show a less diverse range of samples. Make it larger to cover a lot of samples. This plus `--frames` can help determine how fast the video feels.\n","\n","`--random_seed`: this allows you to change your starting place in the z space. Note: this value has nothing to do with the seeds you use to generate images. It just allows you to randomize your start point (and if you want to return to it you can use the same seed multiple times).\n","\n","`--network`: path to your .pkl file\n","\n","`--frames`: How many frames to produce\n","\n","`--fps`: frames per second of your video (24, 30, 48, 60, 120, 144)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GmcpoAdJfWN"},"outputs":[],"source":["!python generate.py \\\n","--outdir=./generated/noiseloop_1003_d1 \\\n","--trunc=0.75 \\\n","--process=\"interpolation\" \\\n","--interpolation=\"noiseloop\" \\\n","--diameter=1 \\\n","--random_seed=1003 \\\n","--network=./pretrained/cat256.pkl \\\n","--frames=120 \\\n","--fps=60"]},{"cell_type":"markdown","metadata":{"id":"PkKFb-4CedOq"},"source":["## 4.5 Circular Loop"]},{"cell_type":"markdown","metadata":{"id":"sFoQYfBuO5Bv"},"source":["The noise loop is, well, noisy. This circular loop will feel much more even, while still providing a random loop.\n","\n","I recommend using a higher `--diameter` value than you do with noise loops. Something between `50.0` and `500.0` alongside `--frames` can help control speed and diversity.\n","\n","**Options**\n","\n","`--outdir`: Output directory (folder) where your images will be saved.\n","\n","`--trunc`: truncation value\n","\n","`--diameter`: This controls how \"wide\" the loop is. Make it smaller to show a less diverse range of samples. Make it larger to cover a lot of samples. This plus `--frames` can help determine how fast the video feels.\n","\n","`--random_seed`: this allows you to change your starting place in the z space. Note: this value has nothing to do with the seeds you use to generate images. It just allows you to randomize your start point (and if you want to return to it you can use the same seed multiple times).\n","\n","`--network`: path to your .pkl file\n","\n","`--frames`: How many frames to produce\n","\n","`--fps`: frames per second of your video (24, 30, 48, 60, 120, 144)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"98U--O29wyB-"},"outputs":[],"source":["!python generate.py \\\n","--outdir=./generated/circularloop_1003_d1000 \\\n","--trunc=0.75 \\\n","--process=\"interpolation\" \\\n","--interpolation=\"circularloop\" \\\n","--diameter=1000 \\\n","--random_seed=1003 \\\n","--network=./pretrained/cat256.pkl \\\n","--frames=120 \\\n","--fps=60"]},{"cell_type":"markdown","metadata":{"id":"7CDdorCE65Dd"},"source":["# Part 5: Projection\n"]},{"cell_type":"markdown","metadata":{"id":"Br_1vrEoQHoB"},"source":["Project images into the latent space. The network looks at your image and tries to reverse engineer its latent vector in order to re-create your image from what the latent space understands about images.\n","\n","**Steps:**\n","1. Find an image you want to project\n","2. Resize it so that it is the same dimensions as your trained model\n","3. Upload the image to your Google Drive in the stylegan2-ada-pytorch repository (in the generated/project/input folder)\n","\n","**Options**\n","\n","`--outdir`: Output directory (folder) where your images will be saved.\n","\n","`--target`: Filepath to your input image\n","\n","`--num-steps`: how many iterations the projector should run for. Lower will mean fewer steps and less likelihood of a good projection. Higher will take longer but will likely produce better images.\n","\n","`--network`: Make sure the `--network` argument points to your .pkl file. (My preferred method is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that into the argument after the `=` sign).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ziPz6ewz6-h3"},"outputs":[],"source":["!python projector.py \\\n","--outdir=./generated/projection/lea6 \\\n","--target=./generated/projection/input/lea256.jpg \\\n","--num-steps=30 \\\n","--seed=20 \\\n","--network=./pretrained/cat256.pkl \\\n","--video-target-concat=True # set to True for side-by-side of target image + output video"]},{"cell_type":"markdown","metadata":{"id":"IRFNacNH60dl"},"source":["# Part 6: Face Alignment"]},{"cell_type":"markdown","metadata":{"id":"M490BUeV64oX"},"source":["*Work in progress, eventually this script will accept a .zip folder.*\n","\n","Align faces using the same process used to align faces for the FFHQ data set. Run this script if you plan to train a data set on faces, or project a face into a latent space with aligned faces like FFHQ, metfaces, or AFHQ (Wild, Dog or Cat) Although this should maybe go in the Data Set Preparation section, I'm including it here at the bottom since it's a more of a niche process that won't be run that frequently."]},{"cell_type":"markdown","metadata":{"id":"5RiMGDfPniUj"},"source":["**Steps**\n","(again, this is a work in progress, so the process is a little less streamlined)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7M2FDtVDoIRy"},"outputs":[],"source":["# 1. change directory into the align_faces folder\n","%cd util/align_faces/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLzBTsxxqCwy"},"outputs":[],"source":["# 2. \"clean\" your images by making sure the alignment script can process them\n","!python clean_images_downscale.py /path/to/dataset.zip 2000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3eIharrBq1KK"},"outputs":[],"source":["# 3. unzip your images to a folder called raw_images\n","!unzip output.zip -d raw_images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vadKricrQMVT"},"outputs":[],"source":["# 4. align the faces, saving them to a folder called aligned\n","!python align_faces.py raw_faces aligned"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifKKwOwUPrw-"},"outputs":[],"source":["#5. (optional) zip up the aligned faces so they can be used for training\n","!zip -r aligned.zip aligned"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ACO9_LcVn06o"},"outputs":[],"source":["# change directory back into the main folder so that all the other scripts work\n","%cd /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch"]},{"cell_type":"markdown","metadata":{"id":"VznRirOE5ENI"},"source":["# Convert Legacy Model\n"]},{"cell_type":"markdown","metadata":{"id":"FoCm6MNAf81S"},"source":["\n","If you have an older version of a model (Tensorflow based StyleGAN, or Runway downloaded .pkl file) you’ll need to convert to the newest version. If you’ve trained in this notebook you do **not** need to use this cell.\n","\n","`--source`: path to model that you want to convert\n","\n","`--dest`: path and file name to convert to."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CzkP-Rww5Np9"},"outputs":[],"source":["!python legacy.py --source=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/cat256.pkl --dest=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/cat256-converted.pkl"]},{"cell_type":"markdown","metadata":{"id":"qywlaS5pgzyH"},"source":["## Combine NPZ files together"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M2VooqrNfIpw"},"outputs":[],"source":["!python combine_npz.py --outdir=/content/npz --npzs='/content/pb-proj1-3-clip.npz,/content/pb-proj1-clip-nocenter.npz,/content/pb-proj2-clip-nocenter.npz,/content/projected_w (2).npz'"]},{"cell_type":"markdown","metadata":{"id":"N4jgk98lPocO"},"source":["# Random snippets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmKbwZDD8gjM"},"outputs":[],"source":["!zip -r vid1.zip /content/out/video1-w-0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uUE4yA2CQKqT"},"outputs":[],"source":["!ffmpeg -y -r 144 -i ./generated/ffhq-144fps/frames/frame%04d.png -vcodec libx265 -crf 25 -preset faster -pix_fmt yuv420p ./generated/ffhq-144fps/lerp144.mp4"]},{"cell_type":"markdown","metadata":{"id":"Yi3d7xzpN2Uj"},"source":["#### 4.4 Spherical Interpolation (Slerp) (Not Working)"]},{"cell_type":"markdown","metadata":{"id":"1HrE0_WkM6qy"},"source":["This gets a little heady, but technically linear interpolations are not the best in high-dimensional GANs. [This github link](https://github.com/soumith/dcgan.torch/issues/14) is one of the more popular explanations and discussions.\n","\n","That said, the differences between lerp and slerp aren't all that significant.\n","\n","Note: Slerp in w space currently isn’t supported.\n","\n","**Options**\n","\n","`--outdir`: Output directory (folder) where your images will be saved.\n","\n","`--trunc`: truncation value\n","\n","`--seeds`: list of seeds to interpolate between. Comma separated. Each set of seeds will be given the same number of frames to complete the interpolation. Set the last seed to the same number as the first to create a seamless looping video.\n","\n","`--network`: path to your .pkl file\n","\n","`--frames`: How many frames to produce per interpolation. (e.g. if you set your frames to **30** and your seeds to 10, 20, 30, 10, this will create a video with **90** frames (10 to 20 (30 frames), 20 to 30 (30 frames), 30 to 10 (30 frames))\n","\n","`--fps`: frames per second of your video (24, 30, 48, 60, 120, 144)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0-cUd3fB_kJ"},"outputs":[],"source":["!python generate.py \\\n","--outdir=./generated/slerp-10_20_30_40_10-z \\\n","--interpolation=\"slerp\" \\\n","--process=\"interpolation\" \\\n","--trunc=0.75 \\\n","--seeds=10,20,30,40,10 \\\n","--network=./pretrained/cat256.pkl \\\n","--frames=120 \\\n","--fps=60"]}],"metadata":{"colab":{"collapsed_sections":["nTxKm73p6Vv1","W7vv9HtN_tWE","-9DH84cXALLF","VBRt41MUBFbE","mi5r1m4uR-xf","bqsU3uta6jzo","OpHg9Y9DEOrC","IB1t-nTSItRZ","ul5PPscSfIj2","LCibYLXYPHPg","aGAdtpujMs6s","Sb1s2hDhSOsf","tPpeQzPudaYC","8zoiE_clkGsc","VjOTCWVonoVL","OSzj0igO8Lfu","uP1HsU_CPcF5","PkKFb-4CedOq","7CDdorCE65Dd","IRFNacNH60dl","VznRirOE5ENI","N4jgk98lPocO","Yi3d7xzpN2Uj"],"provenance":[{"file_id":"https://github.com/dougrosman/stylegan2-ada-pytorch/blob/main/SG2_ADA_PyTorch.ipynb","timestamp":1617234210817}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":0}
